package org.apache.nutch.indexer.more;

import java.text.ParseException;
import java.util.Collection;
import java.util.Date;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.nio.ByteBuffer;
import org.apache.avro.util.Utf8;
import org.apache.commons.lang.time.DateUtils;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.io.Text;
import org.apache.nutch.crawl.CrawlDatum;
import org.apache.nutch.crawl.Inlinks;
import org.apache.nutch.indexer.IndexingException;
import org.apache.nutch.indexer.IndexingFilter_v2x;
import org.apache.nutch.indexer.NutchDocument;
import org.apache.nutch.metadata.HttpHeaders;
import org.apache.nutch.net.protocols.HttpDateFormat;
import org.apache.nutch.parse.Parse;
import org.apache.nutch.storage.WebPage;
import org.apache.nutch.storage.WebPage.Field;
import org.apache.html.dom.HTMLDocumentImpl;
import org.apache.nutch.util.MimeUtil;
import org.apache.oro.text.regex.MalformedPatternException;
import org.apache.oro.text.regex.MatchResult;
import org.apache.oro.text.regex.PatternMatcher;
import org.apache.oro.text.regex.Perl5Compiler;
import org.apache.oro.text.regex.Perl5Matcher;
import org.apache.oro.text.regex.Perl5Pattern;
import org.apache.solr.common.util.DateUtil;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.xml.sax.InputSource;
import org.apache.nutch.util.EncodingDetector;
import java.io.ByteArrayInputStream;
import java.io.IOException;
import java.nio.charset.StandardCharsets;
import org.apache.nutch.util.NutchConfiguration;
import java.util.regex.Matcher;
import java.util.regex.Pattern;
//import org.apache.nutch.parse.ParseStatusUtils;
import org.cyberneko.html.parsers.DOMFragmentParser;
import org.w3c.dom.DOMException;
import org.xml.sax.SAXException;
import org.w3c.dom.DocumentFragment;

/**
 
 * Add (or reset) a few metaData properties as respective fields (if they are
 * available), so that they can be accurately used within the search index.
 * 
 * 'lastModifed' is indexed to support query by date, 'contentLength' obtains
 * content length from the HTTP header, 'type' field is indexed to support query
 * by type and finally the 'title' field is an attempt to reset the title if a
 * content-disposition hint exists. The logic is that such a presence is
 * indicative that the content provider wants the filename therein to be used as
 * the title.
 * 
 * Still need to make content-length searchable!
 * 
 * @author John Xing
 * Modified by Januzs Woyciechowsky
 * Included collection ID
 */

public class MoreIndexingFilter_Custom_Nutch_v2x implements IndexingFilter_v2x {
  public static final Logger LOG = LoggerFactory
      .getLogger(MoreIndexingFilter.class);

  /** Get the MimeTypes resolver instance. */
  private MimeUtil MIME;
  private static final int CHUNK_SIZE = 8192;
  private static Collection<WebPage.Field> FIELDS = new HashSet<WebPage.Field>();

  private static Pattern metaPattern = Pattern.compile(
      "<meta\\s+([^>]*http-equiv=(\"|')?content-type(\"|')?[^>]*)>",
      Pattern.CASE_INSENSITIVE);
  private static Pattern charsetPattern = Pattern.compile(
      "charset=\\s*([a-z][_\\-0-9a-z]*)", Pattern.CASE_INSENSITIVE);
  private static Pattern charsetPatternHTML5 = Pattern.compile(
      "<meta\\s+charset\\s*=\\s*[\"']?([a-z][_\\-0-9a-z]*)[^>]*>",
      Pattern.CASE_INSENSITIVE);
  static {
    FIELDS.add(WebPage.Field.HEADERS);
    FIELDS.add(WebPage.Field.CONTENT_TYPE);
    FIELDS.add(WebPage.Field.MODIFIED_TIME);
    FIELDS.add(WebPage.Field.CONTENT);
  }
  private String parserImpl;
  private DOMContentUtils utils;
  private String defaultCharEncoding;

  @Override
  public NutchDocument filter(NutchDocument doc, String url, WebPage page)
      throws IndexingException {
    addTime(doc, page, url);
    addLength(doc, page, url);
    addType(doc, page, url);
    resetTitle(doc, page, url);
    addCollectionID(doc, page, url);
    if ((page.getContentType().toString()).equalsIgnoreCase("text/html")) {
      getRelevanceContent(doc,page,url);
    }
    return doc;
  }

  // Add time related meta info. Add last-modified if present. Index date as
  // last-modified, or, if that's not present, use fetch time.
  private NutchDocument addTime(NutchDocument doc, WebPage page, String url) {
    long time = -1;
    CharSequence lastModified = page.getHeaders().get(
        new Utf8(HttpHeaders.LAST_MODIFIED));
    // String lastModified = data.getMeta(Metadata.LAST_MODIFIED);
    if (lastModified != null) { // try parse last-modified
      time = getTime(lastModified.toString(), url); // use as time
      String formlastModified = DateUtil.getThreadLocalDateFormat().format(
          new Date(time));
      // store as string
      doc.add("lastModified", formlastModified);
    }

    if (time == -1) { // if no last-modified
      time = page.getModifiedTime(); // use Modified time
    }

    String dateString = DateUtil.getThreadLocalDateFormat().format(
        new Date(time));

    // un-stored, indexed and un-tokenized
    doc.add("date", dateString);

    return doc;
  }

  private long getTime(String date, String url) {
    long time = -1;
    try {
      time = HttpDateFormat.toLong(date);
    } catch (ParseException e) {
      // try to parse it as date in alternative format
      try {
        Date parsedDate = DateUtils.parseDate(date,
            new String[] { "EEE MMM dd HH:mm:ss yyyy",
                "EEE MMM dd HH:mm:ss yyyy zzz", "EEE MMM dd HH:mm:ss zzz yyyy",
                "EEE, dd MMM yyyy HH:mm:ss zzz",
                "EEE,dd MMM yyyy HH:mm:ss zzz", "EEE, dd MMM yyyy HH:mm:sszzz",
                "EEE, dd MMM yyyy HH:mm:ss", "EEE, dd-MMM-yy HH:mm:ss zzz",
                "yyyy/MM/dd HH:mm:ss.SSS zzz", "yyyy/MM/dd HH:mm:ss.SSS",
                "yyyy/MM/dd HH:mm:ss zzz", "yyyy/MM/dd", "yyyy.MM.dd HH:mm:ss",
                "yyyy-MM-dd HH:mm", "MMM dd yyyy HH:mm:ss. zzz",
                "MMM dd yyyy HH:mm:ss zzz", "dd.MM.yyyy HH:mm:ss zzz",
                "dd MM yyyy HH:mm:ss zzz", "dd.MM.yyyy; HH:mm:ss",
                "dd.MM.yyyy HH:mm:ss", "dd.MM.yyyy zzz",
                "yyyy-MM-dd'T'HH:mm:ss'Z'" });
        time = parsedDate.getTime();
        // if (LOG.isWarnEnabled()) {
        // LOG.warn(url + ": parsed date: " + date +" to:"+time);
        // }
      } catch (Exception e2) {
        if (LOG.isWarnEnabled()) {
          LOG.warn(url + ": can't parse erroneous date: " + date);
        }
      }
    }
    return time;
  }

  // Add Content-Length
  private NutchDocument addLength(NutchDocument doc, WebPage page, String url) {
    CharSequence contentLength = page.getHeaders().get(
        new Utf8(HttpHeaders.CONTENT_LENGTH));
    if (contentLength != null) {
      // NUTCH-1010 ContentLength not trimmed
      String trimmed = contentLength.toString().trim();
      if (!trimmed.isEmpty())
        doc.add("contentLength", trimmed);
    }

    return doc;
  }
  // Including functions from parser filter in order to get content without menu
  /**
   * Given a <code>ByteBuffer</code> representing an html file of an
   * <em>unknown</em> encoding, read out 'charset' parameter in the meta tag
   * from the first <code>CHUNK_SIZE</code> bytes. If there's no meta tag for
   * Content-Type or no charset is specified, the content is checked for a
   * Unicode Byte Order Mark (BOM). This will also cover non-byte oriented
   * character encodings (UTF-16 only). If no character set can be determined,
   * <code>null</code> is returned. <br />
   * See also
   * http://www.w3.org/International/questions/qa-html-encoding-declarations,
   * http://www.w3.org/TR/2011/WD-html5-diff-20110405/#character-encoding, and
   * http://www.w3.org/TR/REC-xml/#sec-guessing <br />
   * 
   * @param content
   *          <code>ByteBuffer</code> representation of an html file
   */

  private static String sniffCharacterEncoding(ByteBuffer content) {
    int length = Math.min(content.remaining(), CHUNK_SIZE);

    // We don't care about non-ASCII parts so that it's sufficient
    // to just inflate each byte to a 16-bit value by padding.
    // For instance, the sequence {0x41, 0x82, 0xb7} will be turned into
    // {U+0041, U+0082, U+00B7}.
    String str = new String(content.array(), content.arrayOffset()
        + content.position(), length, StandardCharsets.US_ASCII);

    Matcher metaMatcher = metaPattern.matcher(str);
    String encoding = null;
    if (metaMatcher.find()) {
      Matcher charsetMatcher = charsetPattern.matcher(metaMatcher.group(1));
      if (charsetMatcher.find())
        encoding = new String(charsetMatcher.group(1));
    }
    if (encoding == null) {
      // check for HTML5 meta charset
      metaMatcher = charsetPatternHTML5.matcher(str);
      if (metaMatcher.find()) {
        encoding = new String(metaMatcher.group(1));
      }
    }
    if (encoding == null) {
      // check for BOM
      if (length >= 3 && content.get(0) == (byte) 0xEF
          && content.get(1) == (byte) 0xBB && content.get(2) == (byte) 0xBF) {
        encoding = "UTF-8";
      } else if (length >= 2) {
        if (content.get(0) == (byte) 0xFF && content.get(1) == (byte) 0xFE) {
          encoding = "UTF-16LE";
        } else if (content.get(0) == (byte) 0xFE
            && content.get(1) == (byte) 0xFF) {
          encoding = "UTF-16BE";
        }
      }
    }

    return encoding;
  }
  // get raw content, excluding nav proof concept
  private NutchDocument getRelevanceContent(NutchDocument doc, WebPage page, String url) {
    //CharSequence text = page.getText();
    try {
      DocumentFragment root;
      ByteBuffer contentInOctets = page.getContent();
      InputSource input = new InputSource(new ByteArrayInputStream(
        contentInOctets.array(), contentInOctets.arrayOffset()
            + contentInOctets.position(), contentInOctets.remaining()));
      EncodingDetector detector = new EncodingDetector(conf);
      detector.autoDetectClues(page, true);
      detector.addClue(sniffCharacterEncoding(contentInOctets), "sniffed");
      String encoding = detector.guessEncoding(page, defaultCharEncoding);
      input.setEncoding(encoding);
      root = parse(input);
      StringBuffer sb = new StringBuffer();
      utils.getText(sb, root); // extract text
      doc.removeField("content");

      String content = sb.toString();
      if (MAX_CONTENT_LENGTH > -1 && content.length() > MAX_CONTENT_LENGTH) {
        content = content.substring(0, MAX_CONTENT_LENGTH);
      }
      doc.add("content", content.replaceAll("\\n"," "));
      
    } catch (IOException e) {
      LOG.error("Failed with the following IOException: ", e);
    } catch (DOMException e) {
      LOG.error("Failed with the following DOMException: ", e);
    } catch (Exception e) {
      LOG.error("Failed with the following Exception: ", e);
    }
    return doc;
  }
  private DocumentFragment parse(InputSource input) throws Exception {
    if (parserImpl.equalsIgnoreCase("tagsoup"))
      return parseTagSoup(input);
    else
      return parseNeko(input);
  }

  private DocumentFragment parseTagSoup(InputSource input) throws Exception {
    HTMLDocumentImpl doc = new HTMLDocumentImpl();
    DocumentFragment frag = doc.createDocumentFragment();
    DOMBuilder builder = new DOMBuilder(doc, frag);
    org.ccil.cowan.tagsoup.Parser reader = new org.ccil.cowan.tagsoup.Parser();
    reader.setContentHandler(builder);
    reader.setFeature(org.ccil.cowan.tagsoup.Parser.ignoreBogonsFeature, true);
    reader.setFeature(org.ccil.cowan.tagsoup.Parser.bogonsEmptyFeature, false);
    reader
        .setProperty("http://xml.org/sax/properties/lexical-handler", builder);
    reader.parse(input);
    return frag;
  }

  private DocumentFragment parseNeko(InputSource input) throws Exception {
    DOMFragmentParser parser = new DOMFragmentParser();
    try {
      parser
          .setFeature(
              "http://cyberneko.org/html/features/scanner/allow-selfclosing-iframe",
              true);
      parser.setFeature("http://cyberneko.org/html/features/augmentations",
          true);
      parser.setProperty(
          "http://cyberneko.org/html/properties/default-encoding",
          defaultCharEncoding);
      parser
          .setFeature(
              "http://cyberneko.org/html/features/scanner/ignore-specified-charset",
              true);
      parser
          .setFeature(
              "http://cyberneko.org/html/features/balance-tags/ignore-outside-content",
              false);
      parser.setFeature(
          "http://cyberneko.org/html/features/balance-tags/document-fragment",
          true);
      parser.setFeature(
          "http://cyberneko.org/html/properties/names/elems",
          true);
      parser.setFeature(
          "http://cyberneko.org/html/properties/names/attrs",
          true);
      parser.setFeature("http://cyberneko.org/html/features/augmentations",true);
      parser.setFeature("http://cyberneko.org/html/features/report-errors",
          LOG.isTraceEnabled());
      System.out.println("here");
    } catch (SAXException e) {
    }
    // convert Document to DocumentFragment
    HTMLDocumentImpl doc = new HTMLDocumentImpl();
    doc.setErrorChecking(false);
    DocumentFragment res = doc.createDocumentFragment();
    DocumentFragment frag = doc.createDocumentFragment();
    parser.parse(input, frag);
    res.appendChild(frag);

    try {
      while (true) {
        frag = doc.createDocumentFragment();
        parser.parse(input, frag);
        if (!frag.hasChildNodes())
          break;
        if (LOG.isInfoEnabled()) {
          LOG.info(" - new frag, " + frag.getChildNodes().getLength()
              + " nodes.");
        }
        res.appendChild(frag);
      }
    } catch (Exception x) {
      LOG.error("Failed with the following Exception: ", x);
    }
    ;
    return res;
  }
  // associtated collectionID 
  private NutchDocument addCollectionID(NutchDocument doc, WebPage page, String url) {
    String collectionIDString = null;
    Map<CharSequence,CharSequence> markers  = page.getMarkers();
    for (Map.Entry entry : markers.entrySet()) {
      if(entry.getKey().toString().equals("_collectionID")){
        collectionIDString = entry.getValue().toString();
      }
    }
    String[] collectionIDs = (collectionIDString != null) ? collectionIDString.split(",") : new String[]{"0"};
    for(String collectionID : collectionIDs ){
       doc.add("collectionID", collectionID);
    }   
    //System.out.println(page);
    return doc;
  }

  /**
   * <p>
   * Add Content-Type and its primaryType and subType add contentType,
   * primaryType and subType to field "type" as un-stored, indexed and
   * un-tokenized, so that search results can be confined by contentType or its
   * primaryType or its subType.
   * </p>
   * <p>
   * For example, if contentType is application/vnd.ms-powerpoint, search can be
   * done with one of the following qualifiers
   * type:application/vnd.ms-powerpoint type:application type:vnd.ms-powerpoint
   * all case insensitive. The query filter is implemented in
   * {@link TypeQueryFilter}.
   * </p>
   * 
   * @param doc
   * @param data
   * @param url
   * @return
   */
  private NutchDocument addType(NutchDocument doc, WebPage page, String url) {
    String mimeType = null;
    CharSequence contentType = page.getContentType();
    if (contentType == null)
      contentType = page.getHeaders().get(new Utf8(HttpHeaders.CONTENT_TYPE));
    if (contentType == null) {
      // Note by Jerome Charron on 20050415:
      // Content Type not solved by a previous plugin
      // Or unable to solve it... Trying to find it
      // Should be better to use the doc content too
      // (using MimeTypes.getMimeType(byte[], String), but I don't know
      // which field it is?
      // if (MAGIC) {
      // contentType = MIME.getMimeType(url, content);
      // } else {
      // contentType = MIME.getMimeType(url);
      // }
      mimeType = MIME.getMimeType(url);
    } else {
      mimeType = MIME.forName(MimeUtil.cleanMimeType(contentType.toString()));
    }

    // Checks if we solved the content-type.
    if (mimeType == null) {
      return doc;
    }

    doc.add("type", mimeType);

    // Check if we need to split the content type in sub parts
    if (conf.getBoolean("moreIndexingFilter.indexMimeTypeParts", true)) {
      String[] parts = getParts(mimeType);

      for (String part : parts) {
        doc.add("type", part);
      }
    }

    // leave this for future improvement
    // MimeTypeParameterList parameterList = mimeType.getParameters()

    return doc;
  }

  /**
   * Utility method for splitting mime type into type and subtype.
   * 
   * @param mimeType
   * @return
   */
  static String[] getParts(String mimeType) {
    return mimeType.split("/");
  }

  // Reset title if we see non-standard HTTP header "Content-Disposition".
  // It's a good indication that content provider wants filename therein
  // be used as the title of this url.

  // Patterns used to extract filename from possible non-standard
  // HTTP header "Content-Disposition". Typically it looks like:
  // Content-Disposition: inline; filename="foo.ppt"
  private PatternMatcher matcher = new Perl5Matcher();

  private int MAX_CONTENT_LENGTH;
  private Configuration conf;
  static Perl5Pattern patterns[] = { null, null };
  static {
    Perl5Compiler compiler = new Perl5Compiler();
    try {
      // order here is important
      patterns[0] = (Perl5Pattern) compiler
          .compile("\\bfilename=['\"](.+)['\"]");
      patterns[1] = (Perl5Pattern) compiler.compile("\\bfilename=(\\S+)\\b");
    } catch (MalformedPatternException e) {
      // just ignore
    }
  }

  private NutchDocument resetTitle(NutchDocument doc, WebPage page, String url) {
    CharSequence contentDisposition = page.getHeaders().get(
        new Utf8(HttpHeaders.CONTENT_DISPOSITION));
    if (contentDisposition == null)
      return doc;

    MatchResult result;
    for (int i = 0; i < patterns.length; i++) {
      if (matcher.contains(contentDisposition.toString(), patterns[i])) {
        result = matcher.getMatch();
        doc.removeField("title");
        doc.add("title", result.group(1));
        break;
      }
    }

    return doc;
  }

  public void addIndexBackendOptions(Configuration conf) {
  }

  public void setConf(Configuration conf) {
    this.conf = conf;
    this.parserImpl = getConf().get("parser.html.impl", "neko");
    this.utils = new DOMContentUtils(conf);
    this.defaultCharEncoding = getConf().get(
        "parser.character.encoding.default", "windows-1252");
    MIME = new MimeUtil(conf);
    this.MAX_CONTENT_LENGTH = conf.getInt("indexer.max.content.length", -1);
  }

  public Configuration getConf() {
    return this.conf;
  }

  @Override
  public Collection<Field> getFields() {
    return FIELDS;
  }
}