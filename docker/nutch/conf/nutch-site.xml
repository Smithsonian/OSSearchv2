<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
 Licensed to the Apache Software Foundation (ASF) under one or more
 contributor license agreements.  See the NOTICE file distributed with
 this work for additional information regarding copyright ownership.
 The ASF licenses this file to You under the Apache License, Version 2.0
 (the "License"); you may not use this file except in compliance with
 the License.  You may obtain a copy of the License at
     http://www.apache.org/licenses/LICENSE-2.0
 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an "AS IS" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License.
-->
<!-- Put site-specific property overrides in this file. -->
<configuration>
  <property>
       <name>http.agent.name</name>
       <value>Nutch Spider 1.18</value>
   </property>
  <property>
      <name>db.ignore.external.links</name>
      <value>true</value>
      <description>If true,meta_ outlinks leading from a page to external hosts or domain
          will be ignored. This is an effective way to limit the crawl to include
          only initially injected hosts or domains, without creating complex URLFilters.
          See 'db.ignore.external.links.mode'.
      </description>
  </property>
  <property>
      <name>db.ignore.external.links.mode</name>
      <value>byHost</value>
      <description>Alternative value is byDomain</description>
  </property>
  <property>
      <name>plugin.includes</name>
      <value>protocol-httpclient|urlfilter-regex|index-(basic|more)|query-(basic|site|url|lang)|indexer-solr|nutch-extensionpoints|protocol-httpclient|urlfilter-regex|parse-(text|html|msexcel|msword|mspowerpoint|pdf)|summary-basic|scoring-opic|urlnormalizer-(pass|regex|basic)|urlfilter-regex|parse-(html|tika|metatags)|index-(basic|anchor|more|metadata)</value>
      <description>Regular expression naming plugin directory names to
          include.  Any plugin not matching this expression is excluded.
          By default Nutch includes plugins to crawl HTML and various other
          document formats via HTTP/HTTPS and indexing the crawled content
          into Solr.  More plugins are available to support more indexing
          backends, to fetch ftp:// and file:// URLs, for focused crawling,
          and many other use cases.
      </description>
  </property>
  <property>
      <name>anchorIndexingFilter.deduplicate</name>
      <value>true</value>
      <description>With this enabled the indexer will case-insensitive deduplicate anchors
          before indexing. This prevents possible hundreds or thousands of identical anchors for
          a given page to be indexed but will affect the search scoring (i.e. tf=1.0f).
      </description>
  </property>
  <property>
       <name>file.content.limit</name>
       <value>16000000</value>
      <description>The length limit for downloaded content using the file://
          protocol, in bytes. If this value is non-negative (>=0), content longer
          than it will be truncated; otherwise, no truncation at all. Do not
          confuse this setting with the http.content.limit setting.
      </description>
  </property>
  <property>
       <name>parser.skip.truncated</name>
       <value>false</value>
      <description>Boolean value for whether we should skip parsing for truncated documents. By default this
          property is activated due to extremely high levels of CPU which parsing can sometimes take.
      </description>
  </property>

  <property>
    <name>db.fetch.interval.default</name>
    <value>2592000</value>
    <description>The default number of seconds between re-fetches of a page (30 days).
    </description>
  </property>
  <property>
    <name>db.max.outlinks.per.page</name>
    <value>-1</value>
      <description>The maximum number of outlinks that we'll process for a page.
          If this value is nonnegative (>=0), at most db.max.outlinks.per.page outlinks
          will be processed for a page; otherwise, all outlinks will be processed.
      </description>
  </property>
  <property>
    <name>http.content.limit</name>
    <value>16000000</value>
      <description>The length limit for downloaded content using the http/https
          protocols, in bytes. If this value is non-negative (>=0), content longer
          than it will be truncated; otherwise, no truncation at all. Do not
          confuse this setting with the file.content.limit setting.
      </description>
  </property>

  <!-- parse-metatags plugin properties -->
  <property>
    <name>metatags.names</name>
    <value>*</value>
      <description> Names of the metatags to extract, separated by ',meta_'.
          Use '*' to extract all metatags. Prefixes the names with 'meta_'
          in the parse-metadata. For instance to index description and keywords,
          you need to activate the plugin index-metadata and set the value of the
          parameter 'index.parse.md' to 'meta_description,meta_metatag.keywords'.
      </description>
  </property>
  <property>
    <name>db.update.max.inlinks</name>
    <value>100</value>
      <description>Maximum number of inlinks to take into account when updating
          a URL score in the crawlDB. Only the best scoring inlinks are kept.
      </description>
  </property>
  <property>
    <name>generate.max.count</name>
    <value>1000</value>
      <description>The maximum number of URLs in a single
          fetchlist.  -1 if unlimited. The URLs are counted according
          to the value of the parameter generate.count.mode.
      </description>
  </property>
  <property>
    <name>fetcher.server.delay</name>
    <value>2</value>
      <description>The number of seconds the fetcher will delay between
          successive requests to the same server. Note that this might get
          overridden by a Crawl-Delay from a robots.txt and is used ONLY if
          fetcher.threads.per.queue is set to 1.
      </description>
  </property>
  <property>
    <name>fetcher.threads.fetch</name>
    <value>128</value>
      <description>The number of FetcherThreads the fetcher should use.
          This is also determines the maximum number of requests that are
          made at once (each FetcherThread handles one connection). The total
          number of threads running in distributed mode will be the number of
          fetcher threads * number of nodes as fetcher has one map task per node.
      </description>
  </property>
  <property>
    <name>fetcher.threads.per.queue</name>
    <value>2</value>
      <description>This number is the maximum number of threads that
          should be allowed to access a queue at one time. Setting it to
          a value > 1 will cause the Crawl-Delay value from robots.txt to
          be ignored and the value of fetcher.server.min.delay to be used
          as a delay between successive requests to the same server instead
          of fetcher.server.delay.
      </description>
  </property>
  <property>
    <name>fetcher.timelimit.mins</name>
    <value>45</value>
      <description>This is the number of minutes allocated to the fetching.
          Once this value is reached, any remaining entry from the input URL list is skipped
          and all active queues are emptied. The default value of -1 deactivates the time limit.
      </description>
  </property>

    <!--Not in Nutch 1.18 nutch-default.xml -->
    <property>
        <name>db.max.anchor.length</name>
        <value>3000</value>
        <description>The maximum number of characters permitted in an anchor.
        </description>
    </property>

<!-- index.metadata is not in Nutch 1.x nutch-default.xml, In Nutch 1.x its index.parse.md and each item in the list needs prefixed with 'meta_' -->
  <property>
      <name>index.parse.md</name>
      <value>meta_AlbumTitle,meta_Artist,meta_CatalogNumber,meta_Category,meta_ContentType,meta_Content-Type,meta_Country,meta_Culture,meta_Decade,meta_Format,meta_Genre,meta_HandheldFriendly,meta_ImageURL,meta_InfoText,meta_Instrument,meta_Isrc,meta_Keywords,meta_Label,meta_Language,meta_MobileOptimized,meta_PriceTrackDownload,meta_Program,meta_Region,meta_SampleAudio,meta_SortDate,meta_Source,meta_Subject,meta_SubRegion,meta_Theme,meta_Title,meta_X-UA-Compatible,meta_Year,meta_gEra,meta_inLanguage,meta_useRightsURL,meta_alt_title,meta_author,meta_buylinkalbumdownload,meta_buylinkpre-6cd-book-set,meta_category-site-sections,meta_cleartype,meta_created,meta_cttype,meta_description,meta_generator,meta_keywords,meta_modified,meta_name,meta_physical_description,meta_pricealbumdownload,meta_pricepre-6cd-book-set,meta_record_number,meta_topicids,meta_unit_code,meta_viewport,meta_article:published_time,meta_article:modified_time,meta_dc.author,meta_dc.contributor,meta_dc.creator,meta_dc.date,meta_dc.date.created,meta_dc.format,meta_dc.identifier,meta_dc.publisher,meta_dc.subject,meta_dc.title,meta_dc.type,meta_dcterms.creator,meta_dcterms.date,meta_dcterms.description,meta_dcterms.format,meta_dcterms.identifier,meta_dcterms.ispartof,meta_dcterms.language,meta_dcterms.spatial,meta_dcterms.subject,meta_dcterms.temporal,meta_dcterms.title,meta_dcterms.type,meta_fb:admins,meta_fb:app_id,meta_fb:page_id,meta_og:description,meta_og:image,meta_og:image:url,meta_og:locale,meta_og:site_name,meta_og:title,meta_og:type,meta_og:updated_time,meta_og:url,meta_twitter:card,meta_twitter:creator,meta_twitter:description,meta_twitter:image,meta_twitter:site,meta_twitter:title,meta_twitter:url,meta_Content Type,meta_fid,meta_ftype,meta_Era,meta_Label,meta_Release Decade,meta_sort-lastmodified,meta_sort-title,meta_og:image:alt</value>
      <description>
          Comma-separated list of keys to be taken from the parse metadata to generate fields.
          Can be used e.g. for 'description' or 'keywords' provided that these values are generated
          by a parser (see parse-metatags plugin)
      </description>
    </property>

    <property>
        <name>index.metadata.separator</name>
        <value>\t|\||;</value> <!-- Nutch 1.x code modifications to do regex split -->
        <description>
            Separator to use if you want to index multiple values for a given field. Leave empty to
            treat each value as a single value.
        </description>
    </property>

    <property>
        <name>index.metadata.multivalued.fields</name>
        <value>meta_topicids,meta_Artist,meta_Genre,meta_SubRegion,meta_dc.subject,meta_dcterms.subject,meta_dcterms.spatial,meta_dcterms.temporal,meta_dcterms.creator,meta_Country,meta_Culture,meta_Instrument,meta_Language,meta_Decade,meta_Region,meta_ContentType,meta_Theme,meta_Year,meta_Program,meta_gEra,meta_Format,meta_Content Type,meta_Source,meta_Era,meta_Label,meta_Release Decade</value>
        <description>
            Comma-separated list of dynamic-navigations-multivalue-fields to be taken from the parse metadata to generate fields.
        </description>
    </property>

<!-- Not in Nutch 1.x nutch-default.xml-->
    <property id="fetcher_threads_per_host">
        <name>fetcher.threads.per.host</name>
        <value>4</value>
    </property>


    <!-- N/A for Nutch 1.x-->
    <!--<property>
        <name>gora.buffer.read.limit</name>
        <value>200000</value>
        <description>The maximum number of buffered Records we wish to
            read in one batch. @see org.apache.gora.mapreduce.GoraRecordReader
        </description>
    </property>
    <property>
        <name>gora.buffer.write.limit</name>
        <value>200000</value>
        <description>Configures (for the Hadoop record writer) the maximum number of
            buffered Records we wish to regularly flush to the Gora datastore.
            @see org.apache.gora.mapreduce.GoraRecordWriter.
        </description>
    </property>
    <property>
        <name>storage.data.store.class</name>
        <value>org.apache.gora.mongodb.store.MongoStore</value>
        <description>Default class for storing data</description>
    </property>-->



    <!-- Additional diffs between OSSearch Nutch 2.x nutch-default.xml and Nutch 1.x nutch-default.xml -->
    <property>
        <name>db.ignore.internal.links</name>
        <value>false</value> <!-- Nutch 1.x default -->
        <!--<value>true</value>--> <!-- OSSearch Nutch 2.x setting -->
        <description>If true, outlinks leading from a page to internal hosts or domain
            will be ignored. This is an effective way to limit the crawl to include
            only initially injected hosts or domains, without creating complex URLFilters.
            See 'db.ignore.external.links.mode'.

            *************************************************** NOTE ***************************************************
            Using OSSearch Nutch 2.x setting while testing Nutch 1.x this prevented nutch from crawling anything other then urls in seed file.
            i.e. one url in seed file after 50 rounds only one url was indexed to solr!!!!
            So use the Nutch 1.x default value!
        </description>
    </property>
    <property>
        <name>db.max.outlink.length</name>
        <!--<value>4096</value>--> <!-- Nutch 1.x default -->
        <value>100</value>
        <description>
            The maximum length in characters accepted for outlinks before
            applying URL normalizers and filters.  If this value is
            nonnegative (>=0), only URLs with a length in characters less or
            equal than db.max.outlink.length are accepted and then passed to
            URL normalizers and filters. Doing the length check beforehand
            avoids that normalizers or filters hang up on overlong URLs.
            Note: this property is only used to check URLs found as outlinks
            and redirects, but not for injected URLs.
        </description>
    </property>
</configuration>
